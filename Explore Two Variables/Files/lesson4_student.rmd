Lesson 4
========================================================

***

### Scatterplots and Perceived Audience Size
Notes: After plotting histograms and looking at peoples guesses, we wanted to see how it matched with their actual order size.

**They then turned to scatter plots**.

[Moira's Investigation from Lesson 3](https://classroom.udacity.com/courses/ud651/lessons/755618712/concepts/8140986010923)
[Bernstein, M. S., Bakshy, E., Burke, M., & Karrer, B. (2013). Quantifying the invisible audience in social networks. In Proceedings of the SIGCHI Conference on Human Factors in Computing Systems (CHI 2013), pp. 21-30.](http://hci.stanford.edu/publications/2013/invisibleaudience/invisibleaudience.pdf)
***

### Scatterplots
Notes: We've looked at distributions of single variables and their splitting factors. We will now look at 2 continuous variables at the same time.

**Scatterplot is best to examine the relationship between 2 continuous variables.**

There is not a user interface (UI) feature on Facebook's website that causes people to pick the particular age of 69 or the older ages. In general, it is a good idea to think about what caused certain oddities or values in data sets. This can help explain dirty data.

```{r Scatterplots}
setwd("~/Desktop/Nanodegrees/Data Analyst/Task 4/Data Analysis with R/Explore Two Variables/Files/")
list.files()

library(ggplot2)

pf <- read.csv('pseudo_facebook.tsv', sep='\t')
# You can also read in the data using the following code: 
#read.delim('pseudo_facebook.tsv') 

# These 2 plots are the same. qplot knows x comes first, y comes second.
qplot(x=age, y=friend_count, data = pf)
#qplot(age, friend_count, data = pf)

# The equivalent ggplot syntax for the scatterplot: 
#ggplot(aes(x = age, y = friend_count), data = pf) + 
  #geom_point()


```

***

#### What are some things that you notice right away?
Response: There are lots of data points below the age of 30 with high friend_count, this then decreases normally and peaks again around 70, and again after 90 which is odd.

It looks like younger users have a lot of friends (more than users over the ages of 30). There are some vertical lines in their 70s and 90s which are likely to be fake accounts or teens who have lied about their ages.

***

### ggplot Syntax
Notes: Start by switching to ggplot syntax (more formal and verbose). **ggplot lets us specify more complicated plots.**

**Add 1 layer at a time when building up plots**. This allows you to debug and find broken code.

[ggplot2 geoms](http://docs.ggplot2.org/current/)
[ggplot2 tutorial by Ramon Saccilotto](http://bbs.ceb-institute.org/wp-content/uploads/2011/09/handout_ggplot2.pdf)

```{r ggplot Syntax}
qplot(x=age, y=friend_count, data = pf)

# have to wrap x and y variables in aesthetic wrapper.
ggplot(aes(x=age, y=friend_count), data = pf) + geom_point() +
  xlim(13,90)

# Find min and max limits.
summary(pf$age)

```

***

### Overplotting
Notes: Notice, some of these points are spread out from one another and some are over each other. 

When the points are over each other, it's considered to be **overplotted**. This makes it difficult to tell how many points are in each region. 

We can set the transparency of the points using the **alpha parametr in geom_point**.

After doing that, we can see the bulk of the data lies below the 1000 threshold for friend_count.

**geom_jitter** adds some noise so we get a clearer picture of the relationship between age and friend_count (as age is continuous and not an integer)

```{r Overplotting}
# Setting the alpha parameter. 1/20 means it will take 20 points to be the equivalent of 1 black dot.
ggplot(aes(x=age, y=friend_count), data = pf) +
  geom_jitter(alpha = 1/20) +
  xlim(13,90)
```

#### What do you notice in the plot?
Response: There are a lot of people with friends below 500 in all ages. Below the ages of 25, there are still quite a few people who have friend counts greater than 1000. At around 70, there is an uncharacteristic peak which is likely caused by false account ages.

We can see the friend counts for young users isn't as high as previously thought. There is a peak around 69 and it's fake as it's comparable to users around the 25.26 age group.

***

### Coord_trans()
Notes: We're gonna use a transformation on the y axis so we change the friend_count. We do this to get a better visualisation of the data.

Look up the documentation for [coord_trans](http://docs.ggplot2.org/current/coord_trans.html).


```{r Coord_trans()}
# Looking up coord_trans
?coord_trans
```

#### Look up the documentation for coord_trans() and add a layer to the plot that transforms friend_count using the square root function. Create your plot!

```{r Coord_trans Solution}
# Back to geom_point. If we wanted to add jitter, we'd need more elaborate syntax. Adding noise to 0 creates -ve numbers, which would have imaginary square roots. Fix this by putting position parameter in.
ggplot(aes(x = age, y = friend_count), data = pf) +
  #geom_point(alpha = 1/20) +
  geom_point(alpha = 1/20,  position = position_jitter(h = 0)) +
  xlim(13,90) +
  coord_trans(y = 'sqrt')
```

#### What do you notice?
The friend_count scale changed. It's easier to see the distribution of friend count conditional on age.

Thresholds have friend_count above which there are very few users. 

***

### Alpha and Jitter
Notes: Using knowledge of **alpha** and **jitter**, examine the relationship between friendships_initiated (y) and age (x) using the ggplot syntax.

We recommend building the plot in layers using the **ggplot** syntax, creating a basic scatter plot first to see what the distribution looks like and then adjusting it by adding one layer at a time. 


```{r Alpha and Jitter}
names(pf)

ggplot(aes(x = age, y = friendships_initiated), data = pf) +
  geom_point(alpha = 1/10, position = position_jitter(h = 0)) +
  xlim(13,90) +
  coord_trans(y = 'sqrt')

```

#### What are your observations about your final plot?
People under the age of 30 have a lot of initite a lot more friendships (around 500 more) than people over 30. There is a peak around 69 again which is similar to the peaks around 22-26 and this means it seem fake.

***

### Overplotting and Domain Knowledge
Notes: Moira than took the perceived audience and actual audience, but this time transformed the axis so that it was a % of their friend_count. The plot then showed that most of the points were well below their actual.

***

### Conditional Means
Notes: Sometimes we want to understand how the mean or median of a variable varies with another varaible. That is, it can be helpful to summarise relationships of 2 in other ways, rather than plotting every time.

**dplyr** package lets us split up the data, and apply a function to some parts of the data.

Another warning: Version 0.4.0 of **dplyr** has a bug when using the median function on the summarize layer, depending on the nature of the data being summarized. You may need to cast the data as a numeric (float) type when using it on your local machine, e.g. median(as.numeric(var)).

Learn more about the [dplyr package](https://blog.rstudio.org/2014/01/17/introducing-dplyr/). 

[Introduction to dplyr](http://rstudio-pubs-static.s3.amazonaws.com/11068_8bc42d6df61341b2bed45e9a9a3bf9f4.html) (knitted html file)

The following tutorials are presented by Hadley Wickham at useR 2014:

* [Introduction of dplyr](https://www.r-bloggers.com/hadley-wickham-presents-dplyr-at-user-2014/)
* [dplyr Tutorial Part 1](https://www.r-bloggers.com/hadley-wickhams-dplyr-tutorial-at-user-2014-part-1/)
* [dplyr Tutorial Part 2](https://www.r-bloggers.com/hadley-wickhams-dplyr-tutorial-at-user-2014-part-2/)

There are other ways to work with data and create new data frames without using the dplyr package. Learn about the R functions **lapply**, **tapply**, and **split** in a [blog post](https://rollingyours.wordpress.com/2014/10/20/the-lapply-command-101/).


```{r Conditional Means}
# Package needed to create the table.
# install.packages('dplyr')
library(dplyr)

# Group dataframe by age. We need to pass in a datafrmae/grouping and then save it into a new variable which is passed into the next function.
age_groups <- group_by(pf, age)

# N function can only be in function and it reports how many people are in each group
pf.fc_by_age <- summarise(age_groups, 
          friend_count_mean = mean(friend_count),
          friend_count_median = median(friend_count),
          n = n())

# Arrange by age
pf.fc_by_age <- arrange(pf.fc_by_age)

head(pf.fc_by_age)
```

#### Conditional Means Alternate Code

```{r Conditional Means Alternate Code}
library(dplyr)

# Take dataset and apply function using %>% which allows you to chain functions onto the dataset.
pf.fc_by_age <- pf %>% 
  group_by(age) %>%
  summarise(friend_count_mean = mean(friend_count),
            friend_count_median = median(friend_count),
            n = n()) %>%
  arrange(age)

head(pf.fc_by_age, 20)
```

From the tables above, you can already see that users aged 14-23 have have friend count means and medians that other users. The mean friend_count peaks around 16-17.

#### Create your plot!

Plot mean friend count vs. age using a line graph. Be sure you use the correct variable names and the correct data frame. 

You should be working with the new data frame created from the **dplyr** functions. The data frame is called **'pf.fc_by_age'**.

Use **geom_line()** rather than **geom_point** to create the plot. You can look up the documentation for [geom_line() documentation here](http://docs.ggplot2.org/current/geom_path.html) to see what it does.

```{r Conditional Means Plot}
names(pf.fc_by_age)

ggplot(aes(x = age, y = friend_count_mean),
       data = pf.fc_by_age) + 
  geom_line()
```

***

### Overlaying Summaries with Raw Data
Notes: **ggplot** allows us to easily create various summaries of our data and plot them. This is useful for quick exploration and for combining plots of raw data like our original scatter plot with displayed summaries.

We can help ourselves understand this conditional distribution of friend_counts by also plotting quantiles of the data. 

Note: ggplot 2.0.0 changes the syntax for parameter arguments to functions when using **stat = 'summary'**. To denote parameters that are being set on the function specified by **fun.y**, use the **fun.args** argument, e.g.:
**geom_line(stat = 'summary', fun.y = quantile,          fun.args = list(probs = .9), ... )**

To zoom in, the code should use the **coord_cartesian(xlim = c(13, 90))** layer rather than **xlim(13, 90)** layer.

Try an example and practice problem for calculating [quantiles (percentiles)](http://www.r-tutor.com/elementary-statistics/numerical-measures/percentile).

```{r Overlaying Summaries with Raw Data}
# Plotting Scatter plot with line plot overlayed. Fun.y takes any type of function so we can apply it to the y value.
# Need to set the probability to whatever you want, e.g. could be .5 for 50%, .78 for 78%.
ggplot(aes(x = age, y = friend_count), data = pf) + 
  coord_cartesian(xlim = c(13,70), ylim = c(0,1000)) +
  geom_point(alpha = 0.05,
             position = position_jitter(h = 0),
             color = 'orange') +
  geom_line(stat = 'summary', fun.y = mean) +
  geom_line(stat = 'summary', fun.y = quantile, fun.args = list(probs = .1),
            linetype = 2, color = 'blue') +
  geom_line(stat = 'summary', fun.y = quantile, fun.args = list(probs = .5),
            color = 'blue') +
  geom_line(stat = 'summary', fun.y = quantile, fun.args = list(probs = .9),
            linetype = 2, color = 'blue') 
```

#### What are some of your observations of the plot?
Response: The 90th quantile is less than 1000 for all ages. The 50th quantile is less than the mean. 

For 35 year olds to 60 year olds, the friend count falls below 250. So according to our data, 90% of people in this age group have less than 250 friends.

***

### Moira: Histogram Summary and Scatterplot
See Moira's paper on perceived audience size as well as her final plot and how she aligned the histograms with the scatterplots in the published paper.
[Bernstein, M. S., Bakshy, E., Burke, M., & Karrer, B. (2013). Quantifying the invisible audience in social networks. In Proceedings of the SIGCHI Conference on Human Factors in Computing Systems (CHI 2013), pp. 21-30.](http://hci.stanford.edu/publications/2013/invisibleaudience/invisibleaudience.pdf)

***

### Correlation
Notes: It might be appealing to further summarise the relationship between age and friend count. Rather than having the 4 extra layers of **geom_line**, we could try to summarise the strength of this relationship with a single number. 

A **correlation coefficient** or **Pearson's R**, summaries the strength of a relationship with a single number. 

* Pearsons score of >= 0.3 is meaningful but small.
* Pearsons score of >= 0.5 is moderate.
* Pearsons score of >= 0.7 large.

Correlation coefficients are often denoted with the greek letter rho, in addition to the letter r.

The default method for computing the correlation coefficient is Pearson, and this is true for most statistical software. You do not need to pass the method parameter when calculating the Pearson Product Moment Correlation. 

* [A Visual Guide to Correlation](https://s3.amazonaws.com/udacity-hosted-downloads/ud651/correlation_images.jpeg)
* [Correlation Coefficient](http://www.r-tutor.com/elementary-statistics/numerical-measures/correlation-coefficient)
* [Intro to Inferential Statistics- Correlation](https://classroom.udacity.com/courses/ud201/lessons/1345848540/concepts/1715827370923)

**With** function allows us to evaluate an R expression in an environment constructed from the data.

#### Look up the documentation for the cor.test function.

```{r Correlation}
?cor.test
# Computes Pearson Score
cor.test(pf$age, pf$friend_count, method = 'pearson')

# Alternative way to computer Pearsons Score. 
with(pf, cor.test(age, friend_count, method='pearson'))
```

#### What's the correlation between age and friend count? Round to three decimal places.
Response: -0.0274

***

### Correlation on Subsets
Notes: From the correlation exercise previously, we found that the relationship between age and friend count is not linear. It's not monotonic (increasing or decreasing).

Based on the plot, we may not want to include the older ages in our correlation number as older ages are likely to be incorrect.

What are [monotonic functions](http://en.wikipedia.org/wiki/Monotonic_function)? 

Definition of **ostensibly** - apparently or purportedly, but perhaps not actually.

Use the **subset** command or bracket notation for this exercise. 

```{r Correlation on Subsets}
with(subset(pf, age <= 70), cor.test(age, friend_count))
```

**Remember**: it's important to note that one variable doesn't cause the other. We'd need data from experimental research and make use of inferential statistics rather than descriptive statistics to address causality.

***

### Correlation Methods
Notes: **The Pearson product-moment correlation measures the strength of relationship between any 2 variables.**

**We also have measures of monotonic (either increasing or decreasing) relationships such as a rank correlation measures like Spearman.**

[Correlation Methods: Pearson's r, Spearman's rho, and Kendall's tau](http://www.statisticssolutions.com/correlation-pearson-kendall-spearman/)

```{r Correlation Methods}
with(subset(pf, age <= 70), cor.test(age, friend_count, method = 'spearman'))
```

***

## Create Scatterplots
Notes: Create a scatterplot of likes_received (y) vs. www_likes_received (x). Use any of the techniques that you've learned so far to modify the plot.

```{r Create Scatterplots }
names(pf)

ggplot(aes(www_likes_received, likes_received), data = pf) +
  geom_point()
```

***

### Strong Correlations
Notes: **To determine good x and y limits for our axis, we can look at the 95th percentile using the Quantile command**.

The correlation coefficient is invariant under a linear transformation of either X or Y, and the slope of the regression line when both X and Y have been transformed to z-scores is the correlation coefficient. 

It's important to note that we may not always be interested in the bulk of the data. Sometimes, the outliers ARE of interest, and it's important that we understand their values and why they appear in the data set.

**To add a line of best fit, use geom_smooth**.

```{r Strong Correlations}
# To add a line of best fit, use geom_smooth
ggplot(aes(www_likes_received, likes_received), data = pf) +
  geom_point(alpha = 0.1) +
  coord_cartesian(xlim = c(0, quantile(pf$www_likes_received, 0.95)), 
                  ylim = c(0, quantile(pf$likes_received, 0.95))) +
  geom_smooth(method = 'lm', color = 'red')
```

#### What's the correlation betwen the two variables? Include the top 5% of values for the variable in the calculation and round to 3 decimal places.

```{r Correlation Calcuation}
# Correlation between the 2 variables
with(pf, cor.test(www_likes_received, likes_received, method = 'pearson'))
```

Response: 0.948

***

### Moira on Correlation
Notes: Measure correlation coefficient to quantify how correlated variables are.

This is important because data is correlated e.g. How many status updates in a month is correlated with how many times they logged in that month or how many friends they have or how many photos they have. They measure how engaged someone is.

When you're working on a problem and there's some kind of regression or modelling something, some of these variables go into the regression. **One of the assumptions of the regression is that these variables are independant of each other** so if any 2 are 2 highly correlated with each other, it's difficult to tell which ones are actually driving the phenomenon. 

So measuring the correlation helps you determine which variables you want to throw into the regression and which ones you don't.

[Assumptions of Linear Regression](https://en.wikipedia.org/wiki/Linear_regression#Assumptions)
***

### More Caution with Correlation
Notes: Plotting data is the best way to understand it and can lead you to key insights. Correlation is very useful too.

Argument matching (when not providing them by name) in R is a bit complex.

First, arguments (or parameters) can be matched by name. If a parameter matches exactly, it is "removed" from the argument list and the remaining unnamed arguments are matched in the order that they are listed in the function definition.

R does the following to match arguments:

1. checks for exact match of named argument
2. checks for a partial match of the argument
3. checks for a positional match

If R does not find a match for a parameter, it typically throws an "unused" parameter error.

Type **str(functionName)** to find the order of the parameters and learn more about the parameters of an R function. 

The example covered in the next few videos comes from a practice problem in [Weisberg, S. (2005). Applied Linear Regression, 3rd edition. New York: Wiley](http://users.stat.umn.edu/~sandy/alr3ed/website/).

**The dataset we're using is the Mitchell dataset from the alr3 package.**

```{r More Caution With Correlation}
install.packages('alr3')
library(alr3)

# Loading the dataset. 
data(Mitchell)
?Mitchell
```

#### Create a scatterplot of temperature (Temp) vs. months (Month).

```{r Temp vs Month}
summary(Mitchell)

# Creating scatter plot
ggplot(aes(Month, Temp), data = Mitchell) + 
  geom_point()
```

***

### Noisy Scatterplots
a. Take a guess for the correlation coefficient for the scatterplot.
0

b. What is the actual correlation of the two variables?
(Round to the thousandths place)
0.0575

```{r Noisy Scatterplots}
with(Mitchell, cor.test(Month, Temp, method = 'pearson'))
```

***

### Making Sense of Data
Notes: We know the months variable is very discreet from Jan to Dec and they repeat over again.

As of ggplot 2.0, you will need to use a **scale_x_continuous()** layer instead of **scale_x_discrete()**, since the Month is considered a numeric variable.

```{r Making Sense of Data}
# Find data set range
range(Mitchell$Month)

# Plotting the data with discrete units
ggplot(aes(Month, Temp), data = Mitchell) + 
  geom_point() +
  scale_x_continuous(breaks = seq(0,203,12))
```

***

### A New Perspective

The **cor** and **cor.test** functions determine the strength of a linear relationship, but they may miss other relationships in the data.

#### What do you notice?
Response: It is cyclical every 12 months, like a sine or cosine graph.

#### Watch the solution video and check out the Instructor Notes!
Notes: This is a great example of getting context on your data. There seemed to be little correlation when doing the calculaton however, when looking at the data, you can easily see a correlation.

You could also get perspective on this data by overlaying each year's data on top of each other, giving a clear, generally sinusoidal graph. You can do this by using the R's modulus operator **%%** in your code. Try running the code below!

```{r A New Perspective}
# Overlaying each years data on top of each other
ggplot(aes(x = (Month %% 12), y = Temp), data = Mitchell) + 
  geom_point() 
```

**Data Visualisation Pioneers**:

* [John Tukey](https://en.wikipedia.org/wiki/John_Tukey)
* [William Playfair](https://en.wikipedia.org/wiki/William_Playfair)
* [William Playfair and the Psychology of Graphs](http://www.psych.utoronto.ca/users/spence/Spence%20(2006).pdf)

**Proportion and scale of your graphics do matter. The nature of the data should suggest the shape of the graphic, otherwise you should tend to have a graphic that is 50% wider than tall.**

There are other measures of associations that can detect this. The **dcor.ttest()** function in the **energy** package implements a non-parametric test of the independence of two variables. While the Mitchell soil dataset is too coarse to identify a significant dependency between "Month" and "Temp", we can see the difference between **dcor.ttest** and **cor.test** through other examples, like the following:

```{r}
install.packages('energy')
library(energy)
x <- seq(0, 4*pi, pi/20)
y <- cos(x)
qplot(x = x, y = y)
dcor.ttest(x, y)
```


***

### Understanding Noise: Age to Age Months
Notes: As can be seen in the graph below, there is a lot of random noise (the mean friend count rises and falls over each age). **Noise like this reflects that we just have a sample from the data generating process and so the estimated mean friend count for each age is the true mean plus some noise.**

```{r Understanding Noise: Age to Age Months}
ggplot(aes(age, friend_count_mean), data = pf.fc_by_age) +
  geom_line()

head(pf.fc_by_age, 10)
```

#### Create an age_with_months variable

Create a new variable, **age_with_months**, in the **pf** data frame. Be sure to save the variable in the data frame rather than creating a separate, stand-alone variable. You will need to use the variables **age** and **dob_month** to create the variable **age_with_months**.

The variable **age_with_months** in the data frame **pf** should be a decimal value. For example, the value of **age_with_months** for a 33 year old person born in March would be 33.75 since there are nine months from March to the end of the year. Similarly, someone who was 6 years and 6 months old would have an **age_with_months** value of 6.5.

Assume the reference date for calculating age is December 31, 2013 and that the **age** variable gives age in years at the end of 2013.

```{r}
# Create variable with age in months instead of years. Someone born in March is older than someone in september so 12 - month.
pf$age_with_months <- pf$age + ((12-pf$dob_month)/12)
head(pf)
```

##### Alternate Solutions

```{r }
pf$age_with_months <- pf$age + (1 - pf$dob_month / 12) 

pf$age_with_months <- with(pf, age + (1 - dob_month / 12))
```


***

### Age with Months Means

**Another warning:** Version 0.4.0 of dplyr has a bug when using the median function on the summarize layer, depending on the nature of the data being summarized. You may need to cast the data as a numeric (float) type to get the expected results, e.g. **median(as.numeric(var))**.

#### Programming Assignment

Create a new data frame called **pf.fc_by_age_months** that contains the mean friend count, the median friend count, and the number of users in each group of **age_with_months**. The rows of the data framed should be arranged in increasing order by the **age_with_months** variable.

```{r Age with Months Means}
head(pf$age_with_months)

age_months <- group_by(pf, age_with_months)

# Group dataframe by month. We need to pass in a datafrmae/grouping and then save it into a new variable which is passed into the next function.
pf.fc_by_age_months <- summarise(age_months,
                                 friend_count_mean = mean(friend_count),
                                 friend_count_median = median(friend_count),
                                 n = n())

# Arrange by age_with_months
pf.fc_by_age_months <- arrange(pf.fc_by_age_months)

head(pf.fc_by_age_months)
```

#### Alternate Solution

**Hint 1**: Use the **group_by()**, **summarise()**, and **arrange()** functions in the dplyr package to split the data frame by **age_with_month**. Make sure you arrange by the correct variable (it's not age anymore). 

```{r Alternate Solution}
library(dplyr)

pf.fc_by_age_months <- pf %>%
  group_by(age_with_months) %>%
  summarise(friend_count_mean = mean(friend_count),
            friend_count_median = median(friend_count),
            n = n()) %>%
  arrange(age_with_months)

head(pf.fc_by_age_months)
```

***

### Noise in Conditional Means

Create a new scatterplot showing **friend_count_mean** versus the new variable, **age_with_months**. 

Be sure to use the correct data frame (the one you create in the last exercise) AND subset the data to investigate users with ages less than 71.

```{r Noise in Conditional Means}
ggplot(aes(age_with_months, friend_count_mean),
       data = subset(pf.fc_by_age_months, age_with_months < 71)) +
  geom_line()
```

***

### Smoothing Conditional Means
Notes: We want to put the 2 plots below, side by side so we can look at them together. 

By decreasing the size of our bins and increasing the number of bins, we have less data to estimate each conditional mean. 
On the other hand, we could increase the size of our bins (e.g. age falls on multiple of 5). **Doing this estimates the mean more precisely but we could potentially miss important features of the age and friend count relationship.**

```{r Smoothing Conditional Means}
# Age
p1 <- ggplot(aes(age, friend_count_mean),
       data = subset(pf.fc_by_age, age < 71)) +
  geom_line() +
  geom_smooth()

# Age with months
p2 <- ggplot(aes(age_with_months, friend_count_mean),
       data = subset(pf.fc_by_age_months, age_with_months < 71)) +
  geom_line() +
  geom_smooth()

# Age falls on mulitples of 5. We want to pass mean friend_count so pass summary to stat
p3 <- ggplot(aes(round(age/5) * 5, friend_count),
       data = subset(pf, age < 71)) +
  geom_line(stat = 'summary', fun.y = mean)

library(gridExtra)
grid.arrange(p2,p1,p3, ncol=1)
```

These 3 plots are an example of the **bias variance tradeoff**, and it's similar to the trade off we make when choosing the bin width in histograms. Analysts can better make this tradeoff by using a flexible statistical model to smooth our estimates of conditional means.

By adding **geom_smooth** we can see a line of best fit (which misses key points like the peak at 69). This highlights that using models like **low S** or **smoothing spines** can be useful, but like any model, it can be subject to systematic errors, when the true proces generating our data isn't so consistent with the model itself.

[Local Regression (LOESS)](http://simplystatistics.org/2014/02/13/loess-explained-in-a-gif/) explained visually on the [Simply Statistics](http://simplystatistics.org/) blog.

The Details of [Loess and Lowess](http://en.wikipedia.org/wiki/Local_regression).

***

### Which Plot to Choose?
Notes: **You don't have to choose!**. In EDA, we often create multiple visualisations and summaries of the same data, gleaning different incites from each.

***

### Analyzing Two Variables
Reflection: We covered scatter plots, conditional means and correlation coefficients. 

* We learned how to explore the relationship between 2 variables. Our main visualisation tool was the scatterplot, but we also augmented the scatter plot with conditional summaries, like means.
* We also learned about the benefits and the limitations of using correlation.
* We also learned how correlation may affect your decisions over which variables to include in your final models.
* We learned not to trust our interpretation of initial scatter plots like with the seasonal temp data.
* We learned how to use jitter and transparency to reduce over plotting.
* We learned how to use the dplyr library to create tables to have analysis done on them. 
* We learnt the %>% operator which allows us to chain commands. 
* We learned ggplot in much more depth. 
* We learned that numerical data can give insight, graphs give richer insight but ultimately they are not perfect and we have think about what model we are using to fit the data in order to improve our understanding of the relationships between the variables.

A deep dive into [Bivariate Data Analysis](http://dept.stat.lsa.umich.edu/~kshedden/Courses/Stat401/Notes/401-bivariate-slides.pdf).

***

Click **KnitHTML** to see all of your hard work and to have an html
page of this lesson, your answers, and your notes!

